{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import copy \n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from KoNLI_UTILS import chrome_setting, papago_translation, nan_list_retranslation, hangul_list_retranslation, hangul_word_translation, len_retranslation, pytorch_cos_sim\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE data load\n",
    "json_train_path = 'klue-nli-v1.1_train.json'\n",
    "json_test_path = 'klue-nli-v1.1_dev.json'\n",
    "\n",
    "with open(json_train_path, 'r', encoding=\"utf-8\") as f:\n",
    "    json_train = json.load(f)\n",
    "with open(json_test_path, 'r', encoding=\"utf-8\") as f:\n",
    "    json_test = json.load(f)\n",
    "    \n",
    "json_train_df = pd.DataFrame(json_train)[['premise','hypothesis','gold_label']]\n",
    "json_test_df = pd.DataFrame(json_test)[['premise','hypothesis','gold_label']]        \n",
    "json_train_df.rename(columns = {'gold_label' : 'label'}, inplace = True)\n",
    "json_test_df.rename(columns = {'gold_label' : 'label'}, inplace = True)\n",
    "df1 = pd.concat([json_train_df, json_test_df]).reset_index(drop=True)\n",
    "\n",
    "df_grp1 = df1.groupby(df1.columns.tolist()) # 전체 열 비교\n",
    "df_di1 = df_grp1.groups # 딕셔너리로 만들기 \n",
    "idx_T1 = [x[0] for x in df_di1.values() if len(x) == 1] # 중복X 인덱스 검토\n",
    "idx_F1 = [x[0] for x in df_di1.values() if not len(x) == 1] # 중복O 인덱스 검토\n",
    "df_concated1 = pd.concat([df1.loc[idx_T1,:], df1.loc[idx_F1,:]])\n",
    "df_concated1 = df_concated1.dropna(how='any') # Null 값이 존재하는 행 제거\n",
    "df_concated1 = df_concated1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글 -> 영어 번역\n",
    "* 영어로 번역된 데이터 npy로 저장 후, 번역이 제대로 진행되지 않은 부분을 위해 저장된 npy 로드하여 재번역 시도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['premise', 'hypothesis']:    \n",
    "    print('Col_name : '+column)\n",
    "\n",
    "    set_setting = {'path':'./chromedriver', # your chrome driver path\n",
    "                    'col_name':column,\n",
    "                    'sk':'ko',\n",
    "                    'tk':'en',\n",
    "                    'final_save_name':'to_eng_{}'.format(column)}\n",
    "\n",
    "    path = set_setting['path']\n",
    "    col_name = set_setting['col_name']\n",
    "    sk = set_setting['sk']\n",
    "    tk = set_setting['tk']\n",
    "    final_save_name = set_setting['final_save_name']\n",
    "\n",
    "    driver = chrome_setting(path)\n",
    "    back_translation_file = 'to_eng_{}_0_27996.npy'.format(col_name)\n",
    "    \n",
    "    # to_eng_premise_0_27996.npy, to_eng_hypothesis_0_27996.npy 생성 \n",
    "    papago_translation(df_concated1[col_name], sk, tk, driver, save_name=final_save_name) \n",
    "\n",
    "    raw_array = np.load('{}'.format(back_translation_file))\n",
    "    raw_array_df = pd.DataFrame(raw_array, columns=[col_name])\n",
    "\n",
    "    nan_list = [1]\n",
    "    hangul_ind = [1]\n",
    "    driver.quit()\n",
    "    os.system('killall chrome')\n",
    "    \n",
    "    # retry\n",
    "    for ii in range(2):\n",
    "        raw_array_df, nan_list = nan_list_retranslation(raw_array_df, df_concated1, col_name, sk, tk, path)\n",
    "        raw_array_df, hangul_ind = hangul_list_retranslation(raw_array_df, df_concated1, col_name, sk, tk, path)\n",
    "        raw_array_df = hangul_word_translation(raw_array_df, col_name, sk, tk, path)\n",
    "        raw_array_df = len_retranslation(raw_array_df, df_concated1, col_name, sk, tk, path) \n",
    "    \n",
    "    #  ko -> en 번역된 데이터 csv 파일로 저장. ex) to_eng_premise.csv, to_eng_hypothesis.csv\n",
    "    raw_array_df.to_csv('{}.csv'.format(final_save_name), index=False) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영어 -> 한글 번역\n",
    "* 한글로 번역된 데이터 npy로 저장 후, 번역이 제대로 진행되지 않은 부분을 위해 저장된 npy 로드하여 재번역 시도\n",
    "* 같은 의미의 문장이라도 한글 대비 영어 문장이 길다고 판단되어 길이에 대한 비율에 따른 재번역은 진행 X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['premise', 'hypothesis']:    \n",
    "    print('Col_name : '+column)\n",
    "\n",
    "    set_setting = {'path':'./chromedriver', # your chrome driver path\n",
    "                    'col_name':column,\n",
    "                    'sk':'en',\n",
    "                    'tk':'ko',\n",
    "                    'final_save_name':'to_kor_{}'.format(column)}\n",
    "\n",
    "    path = set_setting['path']\n",
    "    col_name = set_setting['col_name']\n",
    "    sk = set_setting['sk']\n",
    "    tk = set_setting['tk']\n",
    "    final_save_name = set_setting['final_save_name']\n",
    "\n",
    "    driver = chrome_setting(path)\n",
    "    eng_data = pd.read_csv('to_eng_{}_0_27996.csv'.format(col_name))\n",
    "    back_translation_file = 'to_kor_{}_0_27996.npy'.format(col_name)\n",
    "    \n",
    "    # to_kor_premise_0_27996.npy, to_kor_hypothesis_0_27996.npy 생성 \n",
    "    papago_translation(eng_data[col_name], sk, tk, driver, save_name=final_save_name)\n",
    "\n",
    "    raw_array = np.load('{}'.format(back_translation_file))\n",
    "    raw_array_df = pd.DataFrame(raw_array, columns=[col_name])\n",
    "\n",
    "    nan_list = [1]\n",
    "    hangul_ind = [1]\n",
    "    driver.quit()\n",
    "    os.system('killall chrome')\n",
    "    \n",
    "    # retry\n",
    "    for ii in range(2):\n",
    "        raw_array_df, nan_list = nan_list_retranslation(raw_array_df, eng_data, col_name, sk, tk, path)\n",
    "        raw_array_df, hangul_ind = hangul_list_retranslation(raw_array_df, eng_data, col_name, sk, tk, path)\n",
    "        raw_array_df = hangul_word_translation(raw_array_df, col_name, sk, tk, path)\n",
    "    \n",
    "    #  en -> ko 번역된 데이터 csv 파일로 저장. ex) to_kor_premise.csv, to_kor_hypothesis.csv\n",
    "    raw_array_df.to_csv('{}.csv'.format(final_save_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence BERT(SBERT) \n",
    "* 번역된 데이터와 원본 데이터와의 유사도를 구해 유사도가 0.9 이상인 데이터만을 학습에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_premise = pd.read_csv('to_kor_premise.csv')\n",
    "bcak_hypothesis = pd.read_csv('to_kor_hypothesis.csv')\n",
    "\n",
    "sim_data = copy.copy(df_concated1)\n",
    "sim_data['back_premise'] = back_premise\n",
    "sim_data['back_hypothesis'] = bcak_hypothesis\n",
    "\n",
    "sim_data.replace('', np.nan, inplace=True)\n",
    "sim_data.replace(' ', np.nan, inplace=True)\n",
    "nan_list = [index for index, row in sim_data.iterrows() if row.isnull().any()]\n",
    "\n",
    "hangul_ind=[]\n",
    "for i in range(0,len(sim_data)):\n",
    "    temp=re.findall('[a-zA-Z]',str(sim_data.drop(columns=['label']).iloc[i].values))\n",
    "    if len(temp)!=0:\n",
    "        hangul_ind.append(i)\n",
    "        \n",
    "sim_data.drop(index = hangul_ind, inplace=True)\n",
    "sim_data = sim_data[sim_data['back_premise'].apply(lambda x: len(x)>=19)] # 'premise' 글자 수 18 미만인 데이터 제거 \n",
    "sim_data = sim_data[sim_data['back_premise'].apply(lambda x: len(x)<=90)] # 'premise' 글자 수 89 초과인 데이터 제거\n",
    "sim_data = sim_data[sim_data['back_hypothesis'].apply(lambda x: len(x)>=5)] # 'hypothesis' 글자 수 5 미만인 데이터 제거\n",
    "sim_data = sim_data[sim_data['back_hypothesis'].apply(lambda x: len(x)<=103)] # 'hypothesis' 글자 수 103 초과인 데이터 제거\n",
    "\n",
    "sim_data = sim_data.reset_index(drop=True)\n",
    "sim_data[['back_premise', 'back_hypothesis']] = sim_data[['back_premise', 'back_hypothesis']].applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('bespin-global/klue-sentence-roberta-base')\n",
    "model = AutoModel.from_pretrained('bespin-global/klue-sentence-roberta-base')\n",
    "\n",
    "results={}\n",
    "# for col_name in ['premise']:\n",
    "for col_name in ['premise', 'hypothesis']:\n",
    "\n",
    "    # Tokenize sentences\n",
    "    corpus_input = tokenizer(list(sim_data[col_name]), max_length=103, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    queries_input = tokenizer(list(sim_data['back_'+col_name]), max_length=103, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        corpus_output = model(**corpus_input)\n",
    "        queries_output = model(**queries_input)\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling.\n",
    "    corpus_embeddings = mean_pooling(corpus_output, corpus_input['attention_mask'])\n",
    "    queries_embeddings = mean_pooling(queries_output, corpus_input['attention_mask'])\n",
    "    results['corpus'] = corpus_embeddings\n",
    "    results['queries'] = queries_embeddings\n",
    "    \n",
    "    cos_similarity = torch.diagonal(pytorch_cos_sim(results['corpus'], results['queries']), 0)\n",
    "    sim_data[col_name+'_cos'] = cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (sim_data.premise_cos >= 0.9) & (sim_data.hypothesis_cos >= 0.9) # 조건식 작성\n",
    "back_train = sim_data[condition]\n",
    "back_train = back_train.reset_index(drop=True)\n",
    "\n",
    "# Augmentation data save to csv file\n",
    "back_train[['back_premise', 'back_hypothesis', 'label']].to_csv('Augmented_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
