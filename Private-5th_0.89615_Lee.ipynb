{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  4 05:49:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     Off  | 00000000:3B:00.0 Off |                  Off |\n",
      "| 33%   40C    P0    50W / 230W |      0MiB / 16125MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 5000     Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "| 33%   41C    P0    41W / 230W |      0MiB / 16125MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 5000     Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 32%   39C    P0    41W / 230W |      0MiB / 16125MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 5000     Off  | 00000000:D8:00.0 Off |                  Off |\n",
      "| 32%   40C    P0    25W / 230W |      0MiB / 16125MiB |      5%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU activate --> Count of using GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re \n",
    "import copy \n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from googletrans import Translator\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from transformers import logging, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "# Hyperparameter definition\n",
    "seed = 10\n",
    "num_labels = 3\n",
    "KorNLU_num = 50000\n",
    "n_fold = 5\n",
    "epochs = 100\n",
    "patience = 5\n",
    "max_seq_len = 103\n",
    "batch_size = 128\n",
    "weight_decay = 0.0001\n",
    "drop_out = 0.1\n",
    "label_smoothing = 0.2\n",
    "learning_rate = 5e-5\n",
    "max_grad_norm = 10\n",
    "\n",
    "device = '0,1,2,3'\n",
    "text = 'text'\n",
    "model_name = 'klue/roberta-large'\n",
    "library_name = 'AutoModelForSequenceClassification'\n",
    "Model_library = eval(library_name)\n",
    "\n",
    "model_save_folder = './RESULTS/'\n",
    "model_save = '{}_{}_{}_{}_{}_{}_{}_{}_{}_fold'.format(text,\n",
    "                                                    KorNLU_num,\n",
    "                                                    learning_rate,\n",
    "                                                    max_seq_len,\n",
    "                                                    n_fold,\n",
    "                                                    batch_size,\n",
    "                                                    weight_decay,\n",
    "                                                    drop_out,\n",
    "                                                    patience)\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=device\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "print('Device: %s' % device)\n",
    "if (device.type == 'cuda') or (torch.cuda.device_count() > 1):\n",
    "    print('GPU activate --> Count of using GPUs: %s' % torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load\n",
    "\n",
    "* 사용 데이터: KLUE Datasets + KorNLU Datasets\n",
    "\n",
    "* KLUE: https://github.com/KLUE-benchmark/KLUE\n",
    "* KorNLU: https://github.com/kakaobrain/KorNLUDatasets\n",
    "\n",
    "* KLUE, KorNLU 모두 특수문자를 제거하는 경우보다, KorNLU 데이터만 특수문자를 제거하는 경우가 더 성능이 높게 나와 해당 데이터만 특수문자를 제거하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 24426: expected 3 fields, saw 4\\nSkipping line 156343: expected 3 fields, saw 4\\nSkipping line 218766: expected 3 fields, saw 4\\nSkipping line 232318: expected 3 fields, saw 4\\nSkipping line 253493: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 265734: expected 3 fields, saw 4\\nSkipping line 282588: expected 3 fields, saw 4\\nSkipping line 350969: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "# KLUE data load\n",
    "json_train_path = 'klue-nli-v1.1_train.json'\n",
    "json_test_path = 'klue-nli-v1.1_dev.json'\n",
    "\n",
    "with open(json_train_path, 'r', encoding=\"utf-8\") as f:\n",
    "    json_train = json.load(f)\n",
    "with open(json_test_path, 'r', encoding=\"utf-8\") as f:\n",
    "    json_test = json.load(f)\n",
    "    \n",
    "json_train_df = pd.DataFrame(json_train)[['premise','hypothesis','gold_label']]\n",
    "json_test_df = pd.DataFrame(json_test)[['premise','hypothesis','gold_label']]        \n",
    "json_train_df.rename(columns = {'gold_label' : 'label'}, inplace = True)\n",
    "json_test_df.rename(columns = {'gold_label' : 'label'}, inplace = True)\n",
    "df1 = pd.concat([json_train_df, json_test_df]).reset_index(drop=True)\n",
    "\n",
    "df_grp1 = df1.groupby(df1.columns.tolist()) # 전체 열 비교\n",
    "df_di1 = df_grp1.groups # 딕셔너리로 만들기 \n",
    "idx_T1 = [x[0] for x in df_di1.values() if len(x) == 1] # 중복X 인덱스 검토\n",
    "idx_F1 = [x[0] for x in df_di1.values() if not len(x) == 1] # 중복O 인덱스 검토\n",
    "df_concated1 = pd.concat([df1.loc[idx_T1,:], df1.loc[idx_F1,:]])\n",
    "df_concated1 = df_concated1.dropna(how='any') # Null 값이 존재하는 행 제거\n",
    "df_concated1 = df_concated1.reset_index(drop=True)\n",
    "\n",
    "# KorNLU data load\n",
    "multinli_path = 'https://github.com/kakaobrain/KorNLUDatasets/raw/master/KorNLI/multinli.train.ko.tsv'\n",
    "snli_path = 'https://github.com/kakaobrain/KorNLUDatasets/raw/master/KorNLI/snli_1.0_train.ko.tsv'\n",
    "xnli_dev_path = 'https://github.com/kakaobrain/KorNLUDatasets/raw/master/KorNLI/xnli.dev.ko.tsv'\n",
    "xnli_test_path = 'https://github.com/kakaobrain/KorNLUDatasets/raw/master/KorNLI/xnli.test.ko.tsv'\n",
    "    \n",
    "multinli_data = pd.read_csv(multinli_path, sep='\\t', error_bad_lines=False).sample(KorNLU_num)\n",
    "snli_data = pd.read_csv(snli_path, delimiter='\\t', error_bad_lines=False).sample(KorNLU_num)\n",
    "xnli_dev_data = pd.read_csv(xnli_dev_path, delimiter='\\t', error_bad_lines=False)\n",
    "xnli_test_data  = pd.read_csv(xnli_test_path, delimiter='\\t', error_bad_lines=False)\n",
    "\n",
    "multinli_data.rename(columns = {'gold_label' : 'label', 'sentence1' : 'premise', 'sentence2' : 'hypothesis'}, inplace = True)\n",
    "snli_data.rename(columns = {'gold_label' : 'label', 'sentence1' : 'premise', 'sentence2' : 'hypothesis'}, inplace = True)\n",
    "xnli_test_data.rename(columns = {'gold_label' : 'label', 'sentence1' : 'premise', 'sentence2' : 'hypothesis'}, inplace = True)\n",
    "xnli_dev_data.rename(columns = {'gold_label' : 'label', 'sentence1' : 'premise', 'sentence2' : 'hypothesis'}, inplace = True)    \n",
    "df2 = pd.concat([multinli_data,snli_data, xnli_test_data,xnli_dev_data]).reset_index(drop=True)\n",
    "\n",
    "df_grp2 = df2.groupby(df2.columns.tolist()) # 전체 열 비교\n",
    "df_di2 = df_grp2.groups # 딕셔너리로 만들기 \n",
    "idx_T2 = [x[0] for x in df_di2.values() if len(x) == 1] # 중복X 인덱스 검토\n",
    "idx_F2 = [x[0] for x in df_di2.values() if not len(x) == 1] # 중복O 인덱스 검토\n",
    "df_concated2 = pd.concat([df2.loc[idx_T2,:], df2.loc[idx_F2,:]])\n",
    "df_concated2 = df_concated2.dropna(how='any') # Null 값이 존재하는 행 제거\n",
    "\n",
    "df_concated2['premise'] = df_concated2['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9 a-z A-Z]', '', regex=True)\n",
    "df_concated2['hypothesis'] = df_concated2['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9 a-z A-Z]', '', regex=True)\n",
    "\n",
    "df_concated2 = df_concated2[df_concated2['premise'].apply(lambda x: len(x)>=19)] # 'premise' 글자 수 18 미만인 데이터 제거 \n",
    "df_concated2 = df_concated2[df_concated2['premise'].apply(lambda x: len(x)<=90)] # 'premise' 글자 수 89 초과인 데이터 제거\n",
    "df_concated2 = df_concated2[df_concated2['hypothesis'].apply(lambda x: len(x)>=5)] # 'hypothesis' 글자 수 5 미만인 데이터 제거\n",
    "df_concated2 = df_concated2[df_concated2['hypothesis'].apply(lambda x: len(x)<=103)] # 'hypothesis' 글자 수 103 초과인 데이터 제거\n",
    "df_concated2 = df_concated2.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100분간 잘껄 그래도 소닉붐땜에 2점준다</td>\n",
       "      <td>100분간 자는게 더 나았을 것 같다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100분간 잘껄 그래도 소닉붐땜에 2점준다</td>\n",
       "      <td>100분간 잤다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100분간 잘껄 그래도 소닉붐땜에 2점준다</td>\n",
       "      <td>소닉붐이 정말 멋있었다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101빌딩 근처에 나름 즐길거리가 많습니다.</td>\n",
       "      <td>101빌딩 근처에서 즐길거리 찾기는 어렵습니다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101빌딩 근처에 나름 즐길거리가 많습니다.</td>\n",
       "      <td>101빌딩 부근에서는 여러가지를 즐길수 있습니다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27991</th>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 그 어떤 히어로보다 별로다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27992</th>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 액션 장면 진심 그 어떤 히어로보다 멋지다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 진심 최고로 멋지다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>3층 포루는 정면 2칸, 측면 2칸의 팔작 기와지붕으로 벽면 위쪽의 판문에는 전안이...</td>\n",
       "      <td>전안은 벽면 위쪽의 판문에 설치되어 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>오히려 복잡한 람블라스나 카탈루냐보다 낫다고 생각해요.</td>\n",
       "      <td>람블라스나 카탈루냐는 복잡해서 싫어요.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27996 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
       "1                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
       "2                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
       "3                               101빌딩 근처에 나름 즐길거리가 많습니다.   \n",
       "4                               101빌딩 근처에 나름 즐길거리가 많습니다.   \n",
       "...                                                  ...   \n",
       "27991                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
       "27992                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
       "27993                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
       "27994  3층 포루는 정면 2칸, 측면 2칸의 팔작 기와지붕으로 벽면 위쪽의 판문에는 전안이...   \n",
       "27995                     오히려 복잡한 람블라스나 카탈루냐보다 낫다고 생각해요.   \n",
       "\n",
       "                        hypothesis          label  \n",
       "0            100분간 자는게 더 나았을 것 같다.        neutral  \n",
       "1                        100분간 잤다.  contradiction  \n",
       "2                    소닉붐이 정말 멋있었다.        neutral  \n",
       "3       101빌딩 근처에서 즐길거리 찾기는 어렵습니다.  contradiction  \n",
       "4      101빌딩 부근에서는 여러가지를 즐길수 있습니다.     entailment  \n",
       "...                            ...            ...  \n",
       "27991           힛걸 그 어떤 히어로보다 별로다.  contradiction  \n",
       "27992  힛걸 액션 장면 진심 그 어떤 히어로보다 멋지다.        neutral  \n",
       "27993               힛걸 진심 최고로 멋지다.     entailment  \n",
       "27994      전안은 벽면 위쪽의 판문에 설치되어 있다.     entailment  \n",
       "27995        람블라스나 카탈루냐는 복잡해서 싫어요.        neutral  \n",
       "\n",
       "[27996 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>보조금의 75 이상이 다양한 형태의 프로세서 지원을 제공하고 있다</td>\n",
       "      <td>대부분의 사람들은 보조금이 필요하다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 그레이하운드 개가 트랙을 달리고 있습니다</td>\n",
       "      <td>개가 바닥에서 잠을 잔다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어쩌면 울프는 언론에 의해 잘못 이끌렸는지도 모른다 마그넷의 견해에 대한 끈질긴 요약</td>\n",
       "      <td>울프는 울프의 신념에 확고했고 마그넷의 견해에 대한 언론의 요약을 무시했다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101 포와로로부터 전할 말이 있다</td>\n",
       "      <td>나는 너에게 소식을 전하러 왔다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101 포와로로부터 전할 말이 있다</td>\n",
       "      <td>나는 지금 너에게 아무런 소식이 없다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85910</th>\n",
       "      <td>소화기가 트럭 후드에서 연기가 피어오르는 경마장 옆 바리케이드에 앉아 있다</td>\n",
       "      <td>트럭에 불이 붙었다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85911</th>\n",
       "      <td>식당이 있고 테이블은 모두 비어 있다</td>\n",
       "      <td>식당은 문을 닫았다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85912</th>\n",
       "      <td>하얀 개가 수영장으로 뛰어들고 있다</td>\n",
       "      <td>개가 수영장으로 뛰어들고 있다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85913</th>\n",
       "      <td>한 무리의 관중들이 남자 모래 배구 경기를 본다</td>\n",
       "      <td>남자들이 스포츠를 하고 있다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85914</th>\n",
       "      <td>흰색 셔츠와 모자 카키색 바지를 입은 한 골퍼가 손과 클럽을 공중에 들고 잘 해낸 ...</td>\n",
       "      <td>한 골퍼가 마스터컵 우승을 축하하고 있다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85915 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0                   보조금의 75 이상이 다양한 형태의 프로세서 지원을 제공하고 있다   \n",
       "1                               2 그레이하운드 개가 트랙을 달리고 있습니다   \n",
       "2        어쩌면 울프는 언론에 의해 잘못 이끌렸는지도 모른다 마그넷의 견해에 대한 끈질긴 요약   \n",
       "3                                    101 포와로로부터 전할 말이 있다   \n",
       "4                                    101 포와로로부터 전할 말이 있다   \n",
       "...                                                  ...   \n",
       "85910          소화기가 트럭 후드에서 연기가 피어오르는 경마장 옆 바리케이드에 앉아 있다   \n",
       "85911                               식당이 있고 테이블은 모두 비어 있다   \n",
       "85912                                하얀 개가 수영장으로 뛰어들고 있다   \n",
       "85913                         한 무리의 관중들이 남자 모래 배구 경기를 본다   \n",
       "85914  흰색 셔츠와 모자 카키색 바지를 입은 한 골퍼가 손과 클럽을 공중에 들고 잘 해낸 ...   \n",
       "\n",
       "                                      hypothesis          label  \n",
       "0                            대부분의 사람들은 보조금이 필요하다        neutral  \n",
       "1                                  개가 바닥에서 잠을 잔다  contradiction  \n",
       "2      울프는 울프의 신념에 확고했고 마그넷의 견해에 대한 언론의 요약을 무시했다  contradiction  \n",
       "3                              나는 너에게 소식을 전하러 왔다     entailment  \n",
       "4                           나는 지금 너에게 아무런 소식이 없다  contradiction  \n",
       "...                                          ...            ...  \n",
       "85910                                 트럭에 불이 붙었다        neutral  \n",
       "85911                                 식당은 문을 닫았다        neutral  \n",
       "85912                           개가 수영장으로 뛰어들고 있다     entailment  \n",
       "85913                            남자들이 스포츠를 하고 있다     entailment  \n",
       "85914                     한 골퍼가 마스터컵 우승을 축하하고 있다        neutral  \n",
       "\n",
       "[85915 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_concated1); display(df_concated2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Translation\n",
    "\n",
    "* 데이터 증강에 관하여 뉴스 토픽 분류 AI 경진대회 (21.06 ~21.08) Kerry님의 ‘최종 3th : [Private 5위 - 0.83705 / Back Translation]’의 내용을 참고하여 작성하였습니다.\n",
    "* 문장을 번역할 때, papago와 google translator를 사용\n",
    "* 일차적으로 papago를 사용하여 번역을 진행한 후, 번역이 안되는 경우 부가적으로 google translator를 이용하여 번역을 시도\n",
    "* 1-step 번역이 진행됨에 따라 공백 또는 번역이 이루어지지 않는 데이터를 nan값으로 처리 후 재번역 시도\n",
    "* 2-step 번역이 진행됨에 따라 일부분만 번역된 경우 또한 재번역 시도 \n",
    "* 3-step 재번역에도 일부분 번역이 안되는 단어의 경우 문장에서 분리 후 번역기로 해당 단어만 번역 시도\n",
    "* 4-step 한글 -> 영어로 번역 시, 번역된 문장이 기존 문장의 길이에 대한 비율 0.5이하이면 재번역 시도\n",
    "\n",
    "* BackTranslation을 진행할 때 papago API를 이용한 게 아니라 성능이 좋지 못하고 속도도 빠르지않아 굉장히 오래걸립니다. 오류없이 멈추지 않고 진행할 경우 대략 이틀정도 걸리네요 ㅠㅠ\n",
    "Augmentation data는 제 git에 같이 올리도록 할테니, 바로 학습을 진행하실 분들은 Modeling & Prediction 목차부터 진행하시면 될 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 증강을 위한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrome_setting(path):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(path, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def google_translation(dataset, raw_text_list, sk, tk):\n",
    "    raw_trans_list = []\n",
    "    for idx in tqdm(raw_text_list):\n",
    "        translator = Translator()\n",
    "        translator.raise_Exception = True\n",
    "        trans = translator.translate(dataset.iloc[idx], src=sk, dest=tk)\n",
    "        time.sleep(1.5)\n",
    "        raw_trans_list.append(trans.text)\n",
    "    \n",
    "    return raw_trans_list\n",
    "\n",
    "def papago_translation(text_data, sk, tk, driver, save_name=None, index_list=None, mode='save'):\n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "    trans_list = []\n",
    "    \n",
    "    if index_list is not None:\n",
    "        final_index = index_list\n",
    "    else:\n",
    "        final_index = range(len(text_data))\n",
    "    \n",
    "    for idx in tqdm(final_index): \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk='+sk+'&tk='+tk+'&st='+text_data.iloc[idx])\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            backtrans = element.text \n",
    "\n",
    "            if (backtrans=='')|(backtrans==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans = element.text \n",
    "                time.sleep(0.1)\n",
    "                trans_list.append(backtrans)\n",
    "            else:\n",
    "                trans_list.append(backtrans)\n",
    "        except:\n",
    "            trans_list.append('')\n",
    "    \n",
    "        if mode == 'save':\n",
    "            if idx%100==0:\n",
    "                np.save(save_name+'_{}_{}.npy'.format(0,(final_index[-1] + 1)),trans_list)\n",
    "    \n",
    "    driver.close()\n",
    "    driver.quit()  \n",
    "    os.system('killall chrome') \n",
    "\n",
    "    if mode == 'save':    \n",
    "        np.save(save_name+'_{}_{}.npy'.format(0,(final_index[-1] + 1)),trans_list)\n",
    "        print(save_name+'_{}_to_{} is translated!'.format(sk, tk))\n",
    "    else:    \n",
    "        return trans_list\n",
    "    \n",
    "    \n",
    "def nan_list_retranslation(raw_array_df, train, col_name, sk, tk, path):\n",
    "    \n",
    "    print('nan_list re-translation.')\n",
    "    # 번역이 안 된 문장이 존재하는 경우 재번역  \n",
    "    raw_array_df[col_name].replace('', np.nan, inplace=True)\n",
    "    raw_array_df[col_name].replace(' ', np.nan, inplace=True)\n",
    "    nan_list = raw_array_df[raw_array_df[col_name].isnull()].index\n",
    "\n",
    "    if len(nan_list) != 0:\n",
    "        count = 0\n",
    "        \n",
    "        while len(nan_list) != 0:\n",
    "            driver = chrome_setting(path)\n",
    "            if count < 2:\n",
    "                re_trans_list = papago_translation(train[col_name], sk, tk, driver, index_list=nan_list, mode='retry') # 원본 데이터 재번역\n",
    "                \n",
    "            elif count >= 2:\n",
    "                re_trans_list = google_translation(train[col_name], nan_list, sk, tk) # google-translator로 재번역\n",
    "            \n",
    "            for idx, value in zip(nan_list, re_trans_list): \n",
    "                raw_array_df[col_name].iloc[idx] = value   \n",
    "            \n",
    "            driver.quit()\n",
    "            raw_array_df[col_name].replace('', np.nan, inplace=True)\n",
    "            raw_array_df[col_name].replace(' ', np.nan, inplace=True)\n",
    "\n",
    "            nan_list = raw_array_df[raw_array_df[col_name].isnull()].index\n",
    "            count+=1\n",
    "            if count == 4:\n",
    "                break\n",
    "        os.system('killall chrome')\n",
    "        \n",
    "    return raw_array_df, nan_list\n",
    "\n",
    "\n",
    "def hangul_list_retranslation(raw_array_df, train, col_name, sk, tk, path):\n",
    "    \n",
    "    print('hangul_list re-translation.')\n",
    "    # sk -> tk 번역된 문장에 sk가 존재하는 경우 재번역 \n",
    "    hangul_ind=[]\n",
    "    for i in range(0,len(raw_array_df)):\n",
    "        temp=re.findall('[a-zA-Z]',str(raw_array_df[col_name][i]))\n",
    "        if len(temp)!=0:\n",
    "            hangul_ind.append(i)\n",
    "\n",
    "    if len(hangul_ind) != 0:\n",
    "        count = 0\n",
    "        \n",
    "        while len(hangul_ind) != 0:\n",
    "            driver = chrome_setting(path)\n",
    "            \n",
    "            if count < 2:\n",
    "                re_trans_list = papago_translation(train[col_name], sk, tk, driver, index_list=hangul_ind, mode='retry') # 원본 데이터 재번역\n",
    "                            \n",
    "            elif count >= 2:\n",
    "                re_trans_list = google_translation(train[col_name], hangul_ind, sk, tk) # google-translator로 재번역\n",
    "\n",
    "            for idx, value in zip(hangul_ind, re_trans_list): \n",
    "                raw_array_df[col_name].iloc[idx] = value  \n",
    "            \n",
    "            driver.quit()\n",
    "            hangul_ind=[]\n",
    "            for i in range(0,len(raw_array_df)):\n",
    "                temp=re.findall('[a-zA-Z]',str(raw_array_df[col_name][i]))\n",
    "                if len(temp)!=0:\n",
    "                    hangul_ind.append(i)\n",
    "            count+=1\n",
    "            if count == 4:\n",
    "                break\n",
    "            \n",
    "        os.system('killall chrome')\n",
    "        \n",
    "    return raw_array_df, hangul_ind\n",
    "\n",
    "\n",
    "def hangul_word_translation(raw_array_df, col_name, sk, tk, path):\n",
    "    print('hangul_word re-translation.')\n",
    "    hangul_ind=[]\n",
    "    for i in range(0,len(raw_array_df)):\n",
    "        temp=re.findall('[a-zA-Z]',str(raw_array_df[col_name][i]))\n",
    "        if len(temp)!=0:\n",
    "            hangul_ind.append(i)\n",
    "                    \n",
    "    if len(hangul_ind) != 0:\n",
    "        count = 0\n",
    "        while len(hangul_ind) != 0:\n",
    "            \n",
    "            if count < 1:\n",
    "                for idx in tqdm(hangul_ind):\n",
    "                    dictt = {}\n",
    "                    words_raw = re.sub('[^A-Z a-z]', ' ', raw_array_df[col_name].iloc[idx])\n",
    "                    words = words_raw.split(\"  \")\n",
    "                    words = [x.strip() for x in words if x.strip()]\n",
    "                    \n",
    "                    transResult_list = []\n",
    "                    for text in words: \n",
    "                        driver = chrome_setting(path)\n",
    "                        driver.get('https://papago.naver.com/?sk=' + sk + '&tk=' + tk + '&hn=0&st=')\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                        driver.find_element_by_xpath('//*[@id=\"sourceEditArea\"]/label').send_keys(text)\n",
    "                        driver.find_element_by_xpath('//*[@id=\"btnTranslate\"]').click()\n",
    "                        time.sleep(1.5)\n",
    "                        transResult = driver.find_element_by_xpath('//*[@id=\"txtTarget\"]').text\n",
    "                        time.sleep(1)\n",
    "                        transResult_list.append(transResult)\n",
    "                        driver.quit()\n",
    "                        os.system('killall chrome')\n",
    "                        \n",
    "                    dictt['word'] = words\n",
    "                    dictt['translated_word'] = transResult_list\n",
    "\n",
    "                    for i in range(len(dictt['word'])):\n",
    "                        raw_array_df[col_name].iloc[idx] = raw_array_df[col_name].iloc[idx].replace(dictt['word'][i],dictt['translated_word'][i])\n",
    "                \n",
    "            elif count >= 1:\n",
    "                for idx in tqdm(hangul_ind):\n",
    "                    dictt = {}\n",
    "                    words_raw = re.sub('[^A-Z a-z]', ' ', raw_array_df[col_name].iloc[idx])\n",
    "                    words = words_raw.split(\"  \")\n",
    "                    words = [x.strip() for x in words if x.strip()]\n",
    "                    \n",
    "                    transResult_list = []\n",
    "                    for text in words: \n",
    "                        translator = Translator()\n",
    "                        translator.raise_Exception = True\n",
    "                        trans = translator.translate(text, src=sk, dest=tk)\n",
    "                        time.sleep(1.5)\n",
    "                        transResult_list.append(trans.text)\n",
    "                        os.system('killall chrome')\n",
    "                    dictt['word'] = words\n",
    "                    dictt['translated_word'] = transResult_list \n",
    "                    \n",
    "                    for i in range(len(dictt['word'])):\n",
    "                        raw_array_df[col_name].iloc[idx] = raw_array_df[col_name].iloc[idx].replace(dictt['word'][i],dictt['translated_word'][i])\n",
    "                            \n",
    "            hangul_ind=[]\n",
    "            for i in range(0,len(raw_array_df)):\n",
    "                temp=re.findall('[a-zA-Z]',str(raw_array_df[col_name][i]))\n",
    "                if len(temp)!=0:\n",
    "                    hangul_ind.append(i)\n",
    "                    \n",
    "            count+=1\n",
    "            if count >= 2:\n",
    "                break\n",
    "            \n",
    "        os.system('killall chrome')\n",
    "    return raw_array_df\n",
    "\n",
    "def len_retranslation(raw_array_df, train, col_name, sk, tk, path):\n",
    "# Attempt to re-translate a translated sentence if the translated sentence has a ratio of less than 0.5 to the length of an existing sentence\n",
    "    print('len rate re-translation.')    \n",
    "    retrans_ind=[]\n",
    "    for i in range(0,len(raw_array_df)):\n",
    "        if len(raw_array_df[col_name][i])/len(df_concated1[col_name][i])<=0.5:\n",
    "            retrans_ind.append(i)\n",
    "            \n",
    "    retrans_ind=list(set(retrans_ind))\n",
    "    count = 0\n",
    "    \n",
    "    while len(retrans_ind) != 0:\n",
    "        driver = chrome_setting(path)\n",
    "        raw_trans_list = google_translation(train[col_name], retrans_ind, sk, tk)\n",
    "        \n",
    "        for idx, value in zip(retrans_ind, raw_trans_list): \n",
    "            raw_array_df[col_name].iloc[idx] = value  \n",
    "            \n",
    "        retrans_ind=[]\n",
    "        for i in range(0,len(raw_array_df)):\n",
    "            if len(raw_array_df[col_name][i])/len(train[col_name][i])<=0.5:\n",
    "                retrans_ind.append(i)\n",
    "        \n",
    "        retrans_ind=list(set(retrans_ind))   \n",
    "        driver.quit()\n",
    "        \n",
    "        count+=1\n",
    "        if count >= 2:\n",
    "            break\n",
    "        \n",
    "        os.system('killall chrome')\n",
    "    return raw_array_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글 -> 영어 번역\n",
    "* 영어로 번역된 데이터 npy로 저장 후, 번역이 제대로 진행되지 않은 부분을 위해 저장된 npy 로드하여 재번역 시도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col_name : premise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1861/27996 [51:03<11:57:13,  1.65s/it]"
     ]
    }
   ],
   "source": [
    "for column in ['premise', 'hypothesis']:    \n",
    "    print('Col_name : '+column)\n",
    "\n",
    "    set_setting = {'path':'/chromedriver', # your chrome driver path\n",
    "                    'col_name':column,\n",
    "                    'sk':'ko',\n",
    "                    'tk':'en',\n",
    "                    'final_save_name':'to_eng_{}'.format(column)}\n",
    "\n",
    "    path = set_setting['path']\n",
    "    col_name = set_setting['col_name']\n",
    "    sk = set_setting['sk']\n",
    "    tk = set_setting['tk']\n",
    "    final_save_name = set_setting['final_save_name']\n",
    "\n",
    "    driver = chrome_setting(path)\n",
    "    back_translation_file = 'to_eng_{}_0_27996.npy'.format(col_name)\n",
    "    \n",
    "    # to_eng_premise_0_27996.npy, to_eng_hypothesis_0_27996.npy 생성 \n",
    "    papago_translation(df_concated1[col_name], sk, tk, driver, save_name=final_save_name) \n",
    "\n",
    "    raw_array = np.load('{}'.format(back_translation_file))\n",
    "    raw_array_df = pd.DataFrame(raw_array, columns=[col_name])\n",
    "\n",
    "    nan_list = [1]\n",
    "    hangul_ind = [1]\n",
    "    driver.quit()\n",
    "    os.system('killall chrome')\n",
    "    \n",
    "    # retry\n",
    "    for ii in range(2):\n",
    "        raw_array_df, nan_list = nan_list_retranslation(raw_array_df, df_concated1, col_name, sk, tk, path)\n",
    "        raw_array_df, hangul_ind = hangul_list_retranslation(raw_array_df, df_concated1, col_name, sk, tk, path)\n",
    "        raw_array_df = hangul_word_translation(raw_array_df, col_name, sk, tk, path)\n",
    "        raw_array_df = len_retranslation(raw_array_df, df_concated1, col_name, sk, tk, path) \n",
    "    \n",
    "    #  ko -> en 번역된 데이터 csv 파일로 저장. ex) to_eng_premise.csv, to_eng_hypothesis.csv\n",
    "    raw_array_df.to_csv('{}.csv'.format(final_save_name), index=False) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영어 -> 한글 번역\n",
    "* 한글로 번역된 데이터 npy로 저장 후, 번역이 제대로 진행되지 않은 부분을 위해 저장된 npy 로드하여 재번역 시도\n",
    "* 같은 의미의 문장이라도 한글 대비 영어 문장이 길다고 판단되어 길이에 대한 비율에 따른 재번역은 진행 X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['premise', 'hypothesis']:    \n",
    "    print('Col_name : '+column)\n",
    "\n",
    "    set_setting = {'path':'/home/gpuadmin/SY_LEE/chromedriver', # your chrome driver path\n",
    "                    'col_name':column,\n",
    "                    'sk':'en',\n",
    "                    'tk':'ko',\n",
    "                    'final_save_name':'to_kor_{}'.format(column)}\n",
    "\n",
    "    path = set_setting['path']\n",
    "    col_name = set_setting['col_name']\n",
    "    sk = set_setting['sk']\n",
    "    tk = set_setting['tk']\n",
    "    final_save_name = set_setting['final_save_name']\n",
    "\n",
    "    driver = chrome_setting(path)\n",
    "    eng_data = pd.read_csv('to_eng_{}_0_27996.csv'.format(col_name))\n",
    "    back_translation_file = 'to_kor_{}_0_27996.npy'.format(col_name)\n",
    "    \n",
    "    # to_kor_premise_0_27996.npy, to_kor_hypothesis_0_27996.npy 생성 \n",
    "    papago_translation(eng_data[col_name], sk, tk, driver, save_name=final_save_name)\n",
    "\n",
    "    raw_array = np.load('{}'.format(back_translation_file))\n",
    "    raw_array_df = pd.DataFrame(raw_array, columns=[col_name])\n",
    "\n",
    "    nan_list = [1]\n",
    "    hangul_ind = [1]\n",
    "    driver.quit()\n",
    "    os.system('killall chrome')\n",
    "    \n",
    "    # retry\n",
    "    for ii in range(2):\n",
    "        raw_array_df, nan_list = nan_list_retranslation(raw_array_df, eng_data, col_name, sk, tk, path)\n",
    "        raw_array_df, hangul_ind = hangul_list_retranslation(raw_array_df, eng_data, col_name, sk, tk, path)\n",
    "        raw_array_df = hangul_word_translation(raw_array_df, col_name, sk, tk, path)\n",
    "    \n",
    "    #  en -> ko 번역된 데이터 csv 파일로 저장. ex) to_kor_premise.csv, to_kor_hypothesis.csv\n",
    "    raw_array_df.to_csv('{}.csv'.format(final_save_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence BERT(SBERT) \n",
    "* 번역된 데이터와 원본 데이터와의 유사도를 구해 유사도가 0.9 이상인 데이터만을 학습에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_cos_sim(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    return cos_sim(a, b)\n",
    "\n",
    "def cos_sim(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_premise = pd.read_csv('to_kor_premise.csv')\n",
    "bcak_hypothesis = pd.read_csv('to_kor_hypothesis.csv')\n",
    "\n",
    "sim_data = copy.copy(df_concated1)\n",
    "sim_data['back_premise'] = back_premise\n",
    "sim_data['back_hypothesis'] = bcak_hypothesis\n",
    "\n",
    "sim_data.replace('', np.nan, inplace=True)\n",
    "sim_data.replace(' ', np.nan, inplace=True)\n",
    "nan_list = [index for index, row in sim_data.iterrows() if row.isnull().any()]\n",
    "\n",
    "hangul_ind=[]\n",
    "for i in range(0,len(sim_data)):\n",
    "    temp=re.findall('[a-zA-Z]',str(sim_data.drop(columns=['label']).iloc[i].values))\n",
    "    if len(temp)!=0:\n",
    "        hangul_ind.append(i)\n",
    "        \n",
    "sim_data.drop(index = hangul_ind, inplace=True)\n",
    "sim_data = sim_data[sim_data['back_premise'].apply(lambda x: len(x)>=19)] # 'premise' 글자 수 18 미만인 데이터 제거 \n",
    "sim_data = sim_data[sim_data['back_premise'].apply(lambda x: len(x)<=90)] # 'premise' 글자 수 89 초과인 데이터 제거\n",
    "sim_data = sim_data[sim_data['back_hypothesis'].apply(lambda x: len(x)>=5)] # 'hypothesis' 글자 수 5 미만인 데이터 제거\n",
    "sim_data = sim_data[sim_data['back_hypothesis'].apply(lambda x: len(x)<=103)] # 'hypothesis' 글자 수 103 초과인 데이터 제거\n",
    "\n",
    "sim_data = sim_data.reset_index(drop=True)\n",
    "sim_data[['back_premise', 'back_hypothesis']] = sim_data[['back_premise', 'back_hypothesis']].applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>back_premise</th>\n",
       "      <th>back_hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100분간 잘껄 그래도 소닉붐땜에 2점준다</td>\n",
       "      <td>100분간 자는게 더 나았을 것 같다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.</td>\n",
       "      <td>100분만 자는 게 더 좋았을 것 같아.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100분간 잘껄 그래도 소닉붐땜에 2점준다</td>\n",
       "      <td>100분간 잤다.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.</td>\n",
       "      <td>100분 잤어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100분간 잘껄 그래도 소닉붐땜에 2점준다</td>\n",
       "      <td>소닉붐이 정말 멋있었다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.</td>\n",
       "      <td>소닉 붐은 정말로 시원했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101빌딩 근처에 나름 즐길거리가 많습니다.</td>\n",
       "      <td>101빌딩 근처에서 즐길거리 찾기는 어렵습니다.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>101빌딩 근처에는 즐길 거리가 많습니다.</td>\n",
       "      <td>101빌딩 근처에서는 즐길 거리를 찾기가 힘드네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101빌딩 근처에 나름 즐길거리가 많습니다.</td>\n",
       "      <td>101빌딩 부근에서는 여러가지를 즐길수 있습니다.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>101빌딩 근처에는 즐길 거리가 많습니다.</td>\n",
       "      <td>101빌딩 근처에서는 많은 것을 즐길 수 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27010</th>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 그 어떤 히어로보다 별로다.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.</td>\n",
       "      <td>히트 소녀는 다른 영웅보다 더 나쁩니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27011</th>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 액션 장면 진심 그 어떤 히어로보다 멋지다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.</td>\n",
       "      <td>히트걸 액션 장면은 다른 어떤 영웅보다 정말 멋지다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27012</th>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 진심 최고로 멋지다.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.</td>\n",
       "      <td>소녀는 심각하게 가장 멋지다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27013</th>\n",
       "      <td>3층 포루는 정면 2칸, 측면 2칸의 팔작 기와지붕으로 벽면 위쪽의 판문에는 전안이...</td>\n",
       "      <td>전안은 벽면 위쪽의 판문에 설치되어 있다.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>3층 포루는 팔작지붕으로 정면 2칸 측면 2칸이며, 벽면 상부에 정면 조망이 설치되...</td>\n",
       "      <td>전면은 벽 위 패널에 설치되어 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27014</th>\n",
       "      <td>오히려 복잡한 람블라스나 카탈루냐보다 낫다고 생각해요.</td>\n",
       "      <td>람블라스나 카탈루냐는 복잡해서 싫어요.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>나는 그것이 복잡한 람블라스나 카탈루냐보다는 낫다고 생각한다.</td>\n",
       "      <td>나는 람블라스나 카탈루냐가 복잡해서 싫어.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27015 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
       "1                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
       "2                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
       "3                               101빌딩 근처에 나름 즐길거리가 많습니다.   \n",
       "4                               101빌딩 근처에 나름 즐길거리가 많습니다.   \n",
       "...                                                  ...   \n",
       "27010                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
       "27011                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
       "27012                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
       "27013  3층 포루는 정면 2칸, 측면 2칸의 팔작 기와지붕으로 벽면 위쪽의 판문에는 전안이...   \n",
       "27014                     오히려 복잡한 람블라스나 카탈루냐보다 낫다고 생각해요.   \n",
       "\n",
       "                        hypothesis          label  \\\n",
       "0            100분간 자는게 더 나았을 것 같다.        neutral   \n",
       "1                        100분간 잤다.  contradiction   \n",
       "2                    소닉붐이 정말 멋있었다.        neutral   \n",
       "3       101빌딩 근처에서 즐길거리 찾기는 어렵습니다.  contradiction   \n",
       "4      101빌딩 부근에서는 여러가지를 즐길수 있습니다.     entailment   \n",
       "...                            ...            ...   \n",
       "27010           힛걸 그 어떤 히어로보다 별로다.  contradiction   \n",
       "27011  힛걸 액션 장면 진심 그 어떤 히어로보다 멋지다.        neutral   \n",
       "27012               힛걸 진심 최고로 멋지다.     entailment   \n",
       "27013      전안은 벽면 위쪽의 판문에 설치되어 있다.     entailment   \n",
       "27014        람블라스나 카탈루냐는 복잡해서 싫어요.        neutral   \n",
       "\n",
       "                                            back_premise  \\\n",
       "0                       100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.   \n",
       "1                       100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.   \n",
       "2                       100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.   \n",
       "3                                101빌딩 근처에는 즐길 거리가 많습니다.   \n",
       "4                                101빌딩 근처에는 즐길 거리가 많습니다.   \n",
       "...                                                  ...   \n",
       "27010                        네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.   \n",
       "27011                        네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.   \n",
       "27012                        네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.   \n",
       "27013  3층 포루는 팔작지붕으로 정면 2칸 측면 2칸이며, 벽면 상부에 정면 조망이 설치되...   \n",
       "27014                 나는 그것이 복잡한 람블라스나 카탈루냐보다는 낫다고 생각한다.   \n",
       "\n",
       "                     back_hypothesis  \n",
       "0             100분만 자는 게 더 좋았을 것 같아.  \n",
       "1                          100분 잤어요.  \n",
       "2                    소닉 붐은 정말로 시원했다.  \n",
       "3       101빌딩 근처에서는 즐길 거리를 찾기가 힘드네요.  \n",
       "4       101빌딩 근처에서는 많은 것을 즐길 수 있습니다.  \n",
       "...                              ...  \n",
       "27010         히트 소녀는 다른 영웅보다 더 나쁩니다.  \n",
       "27011  히트걸 액션 장면은 다른 어떤 영웅보다 정말 멋지다.  \n",
       "27012               소녀는 심각하게 가장 멋지다.  \n",
       "27013         전면은 벽 위 패널에 설치되어 있습니다.  \n",
       "27014        나는 람블라스나 카탈루냐가 복잡해서 싫어.  \n",
       "\n",
       "[27015 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('bespin-global/klue-sentence-roberta-base')\n",
    "model = AutoModel.from_pretrained('bespin-global/klue-sentence-roberta-base')\n",
    "\n",
    "results={}\n",
    "# for col_name in ['premise']:\n",
    "for col_name in ['premise', 'hypothesis']:\n",
    "\n",
    "    # Tokenize sentences\n",
    "    corpus_input = tokenizer(list(sim_data[col_name]), max_length=103, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    queries_input = tokenizer(list(sim_data['back_'+col_name]), max_length=103, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        corpus_output = model(**corpus_input)\n",
    "        queries_output = model(**queries_input)\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling.\n",
    "    corpus_embeddings = mean_pooling(corpus_output, corpus_input['attention_mask'])\n",
    "    queries_embeddings = mean_pooling(queries_output, corpus_input['attention_mask'])\n",
    "    results['corpus'] = corpus_embeddings\n",
    "    results['queries'] = queries_embeddings\n",
    "    \n",
    "    cos_similarity = torch.diagonal(pytorch_cos_sim(results['corpus'], results['queries']), 0)\n",
    "    sim_data[col_name+'_cos'] = cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 premise  \\\n",
      "0                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
      "1                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
      "2                                100분간 잘껄 그래도 소닉붐땜에 2점준다   \n",
      "3                               101빌딩 근처에 나름 즐길거리가 많습니다.   \n",
      "4                               101빌딩 근처에 나름 즐길거리가 많습니다.   \n",
      "...                                                  ...   \n",
      "27010                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
      "27011                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
      "27012                           힛걸 진심 최고다 그 어떤 히어로보다 멋지다   \n",
      "27013  3층 포루는 정면 2칸, 측면 2칸의 팔작 기와지붕으로 벽면 위쪽의 판문에는 전안이...   \n",
      "27014                     오히려 복잡한 람블라스나 카탈루냐보다 낫다고 생각해요.   \n",
      "\n",
      "                                            back_premise  premise_cos  \n",
      "0                       100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.     0.761380  \n",
      "1                       100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.     0.761380  \n",
      "2                       100분 동안 더 잘할 걸 그랬어. 소닉붐 2점 드릴게요.     0.761380  \n",
      "3                                101빌딩 근처에는 즐길 거리가 많습니다.     0.969300  \n",
      "4                                101빌딩 근처에는 즐길 거리가 많습니다.     0.969300  \n",
      "...                                                  ...          ...  \n",
      "27010                        네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.     0.758181  \n",
      "27011                        네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.     0.758181  \n",
      "27012                        네가 최고야. 당신은 그 어떤 영웅보다도 멋져요.     0.758181  \n",
      "27013  3층 포루는 팔작지붕으로 정면 2칸 측면 2칸이며, 벽면 상부에 정면 조망이 설치되...     0.788977  \n",
      "27014                 나는 그것이 복잡한 람블라스나 카탈루냐보다는 낫다고 생각한다.     0.945114  \n",
      "\n",
      "[27015 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "display(sim_data[['premise','back_premise','premise_cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back_premise</th>\n",
       "      <th>back_hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101빌딩 근처에는 즐길 거리가 많습니다.</td>\n",
       "      <td>101빌딩 근처에서는 즐길 거리를 찾기가 힘드네요.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101빌딩 근처에는 즐길 거리가 많습니다.</td>\n",
       "      <td>101빌딩 주변에는 젊은이들이 즐길 수 있는 것들이 많다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>웬디는 10년 만에 찾는 피터에게 따뜻하게 인사하고 피터는 성공적으로 연설을 마쳤고...</td>\n",
       "      <td>웬디는 피터에게 차갑게 인사했다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>웬디는 10년 만에 찾는 피터에게 따뜻하게 인사하고 피터는 성공적으로 연설을 마쳤고...</td>\n",
       "      <td>잭과 매기는 피터 배닝의 형제이다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>웬디는 10년 만에 찾는 피터에게 따뜻하게 인사하고 피터는 성공적으로 연설을 마쳤고...</td>\n",
       "      <td>피터 배닝, 잭, 그리고 매기는 형제입니다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>희영은 아내와 함께 세상을 떠난 뒤 아들과 단둘이 살고 있는 성현에 설렘을 느낀다.</td>\n",
       "      <td>성현은 아내와 이혼하고 혼자 살고 있습니다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8675</th>\n",
       "      <td>희영은 아내와 함께 세상을 떠난 뒤 아들과 단둘이 살고 있는 성현에 설렘을 느낀다.</td>\n",
       "      <td>성현이의 아내가 죽었어요.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8676</th>\n",
       "      <td>희영은 아내와 함께 세상을 떠난 뒤 아들과 단둘이 살고 있는 성현에 설렘을 느낀다.</td>\n",
       "      <td>희영 씨도 남편과 사별했습니다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8677</th>\n",
       "      <td>힐링푸드 전문가 양성 참여, 개발식품 활용 등의 혜택도 제공된다.</td>\n",
       "      <td>힐링 푸드 전문가들에게 혜택이 주어진다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8678</th>\n",
       "      <td>나는 그것이 복잡한 람블라스나 카탈루냐보다는 낫다고 생각한다.</td>\n",
       "      <td>나는 람블라스나 카탈루냐가 복잡해서 싫어.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8679 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           back_premise  \\\n",
       "0                               101빌딩 근처에는 즐길 거리가 많습니다.   \n",
       "1                               101빌딩 근처에는 즐길 거리가 많습니다.   \n",
       "2     웬디는 10년 만에 찾는 피터에게 따뜻하게 인사하고 피터는 성공적으로 연설을 마쳤고...   \n",
       "3     웬디는 10년 만에 찾는 피터에게 따뜻하게 인사하고 피터는 성공적으로 연설을 마쳤고...   \n",
       "4     웬디는 10년 만에 찾는 피터에게 따뜻하게 인사하고 피터는 성공적으로 연설을 마쳤고...   \n",
       "...                                                 ...   \n",
       "8674     희영은 아내와 함께 세상을 떠난 뒤 아들과 단둘이 살고 있는 성현에 설렘을 느낀다.   \n",
       "8675     희영은 아내와 함께 세상을 떠난 뒤 아들과 단둘이 살고 있는 성현에 설렘을 느낀다.   \n",
       "8676     희영은 아내와 함께 세상을 떠난 뒤 아들과 단둘이 살고 있는 성현에 설렘을 느낀다.   \n",
       "8677               힐링푸드 전문가 양성 참여, 개발식품 활용 등의 혜택도 제공된다.   \n",
       "8678                 나는 그것이 복잡한 람블라스나 카탈루냐보다는 낫다고 생각한다.   \n",
       "\n",
       "                       back_hypothesis          label  \n",
       "0         101빌딩 근처에서는 즐길 거리를 찾기가 힘드네요.  contradiction  \n",
       "1     101빌딩 주변에는 젊은이들이 즐길 수 있는 것들이 많다.        neutral  \n",
       "2                   웬디는 피터에게 차갑게 인사했다.  contradiction  \n",
       "3                  잭과 매기는 피터 배닝의 형제이다.        neutral  \n",
       "4             피터 배닝, 잭, 그리고 매기는 형제입니다.     entailment  \n",
       "...                                ...            ...  \n",
       "8674          성현은 아내와 이혼하고 혼자 살고 있습니다.  contradiction  \n",
       "8675                    성현이의 아내가 죽었어요.     entailment  \n",
       "8676                 희영 씨도 남편과 사별했습니다.        neutral  \n",
       "8677            힐링 푸드 전문가들에게 혜택이 주어진다.        neutral  \n",
       "8678           나는 람블라스나 카탈루냐가 복잡해서 싫어.        neutral  \n",
       "\n",
       "[8679 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "condition = (sim_data.premise_cos >= 0.9) & (sim_data.hypothesis_cos >= 0.9) # 조건식 작성\n",
    "back_train = sim_data[condition]\n",
    "back_train = back_train.reset_index(drop=True)\n",
    "display(back_train[['back_premise', 'back_hypothesis', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation data save to csv file\n",
    "back_train[['back_premise', 'back_hypothesis', 'label']].to_csv('Augmented_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Prediction\n",
    "\n",
    "### Definition functions to train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_len=128, mode='train'):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            record = self.data[index]\n",
    "            encoding_result = self.tokenizer.encode_plus(record['premise'], record['hypothesis'], \n",
    "                                                    max_length=self.max_seq_len, \n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    truncation=True)\n",
    "            return {'input_ids': np.array(encoding_result['input_ids'], dtype=int),\n",
    "                    'attention_mask': np.array(encoding_result['attention_mask'], dtype=int),\n",
    "                    'token_type_ids': np.array(encoding_result['token_type_ids'], dtype=int),\n",
    "                    'labels': np.array(record['label_num'], dtype=int)}\n",
    "            \n",
    "        else:\n",
    "            record = self.data.iloc[index]\n",
    "            encoding_result = self.tokenizer.encode_plus(record['premise'], record['hypothesis'], \n",
    "                                                    max_length=self.max_seq_len, \n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    truncation=True)\n",
    "            return {'input_ids': np.array(encoding_result['input_ids'], dtype=int),\n",
    "                    'attention_mask': np.array(encoding_result['attention_mask'], dtype=int),\n",
    "                    'token_type_ids': np.array(encoding_result['token_type_ids'], dtype=int)}\n",
    "\n",
    "\n",
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth_one_hot(targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
    "                    device=targets.device) \\\n",
    "                .fill_(smoothing /(n_classes-1)) \\\n",
    "                .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        lsm = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            lsm = lsm * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = -(targets * lsm).sum(-1)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)\n",
    "\n",
    "\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def predict(models,dataset,device):\n",
    "    results = []\n",
    "    tqdm_dataset = tqdm(enumerate(dataset), total=len(dataset))\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        ids = torch.tensor(batch_item['input_ids'], dtype=torch.long, device=device)\n",
    "        segment_ids = torch.tensor(batch_item['token_type_ids'], dtype=torch.long, device=device)\n",
    "        atts = torch.tensor(batch_item['attention_mask'], dtype=torch.float, device=device)\n",
    "        \n",
    "        for fold,model in enumerate(models):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if fold == 0:\n",
    "                    pred = model(input_ids=ids, token_type_ids=segment_ids, attention_mask=atts)[0] \n",
    "                else:\n",
    "                    pred = pred + model(input_ids=ids, token_type_ids=segment_ids, attention_mask=atts)[0] \n",
    "        pred = 0.2*pred\n",
    "        pred = torch.tensor(torch.argmax(pred, axis=-1), dtype=torch.int32).cpu().numpy()\n",
    "        results.extend(pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Validation \n",
    "\n",
    "* 모델은 klue_roberta_large를 사용했습니다. 5-fold를 이용하여 각 80%의 train data로 학습시킨 모델 5개로 test셋에 대해 soft voting ensemble 한 것을 최종 파일로 제출했습니다. \n",
    "* 학습 데이터: KLUE(80%) + KorNLU + Back Translation\n",
    "* 검증 데이터: KLUE(20%)\n",
    "\n",
    "* Optimizer 는 AdamW를 사용했으며, amp.GradScaler를 적용하였습니다. 또한, 모델이 과적합을 한다고 판단해 LabelSmoothing을 적용했습니다. 해당 기법 적용시 약소한 성능 향상이 있었으며,\n",
    "각 fold마다 EarlyStopping을 사용하여 fold별 모델을 선택하였습니다.\n",
    "\n",
    "\n",
    "* 실험은 진행하는 도중 loss는 보지 않고 acc를 위주로 보고 판단을 하였는데, acc가 높은 모델을 선택하더라도 LB에서는 오히려 더 낮게 나오는 경우가 존재했습니다.\n",
    "실제로 제가 제출한 모델에서 1-fold: 10 epoch, 2-fold: 11 epoch, 3-fold: 9 epoch, 4-fold: 14 epoch, 5-fold: 14 epoch로 최종 학습한 모델을 사용한 경우 LB score는 0.89였는데, \n",
    "5-fold: 14->10 epoch으로 바꿔서 선택한 경우 LB score는 0.903으로 증가한 성능을 보였습니다.\n",
    "1,2,3,4 fold에 대해서 시간관계상 진행을 못했지만, loss를 보고 판단했다면 더 좋은 성능을 보이지 않았을까 하는 생각이 있습니다.\n",
    "번외로, EarlyStopping의 patience를 더 줄여서 사용하는 것도 더 좋은 성능으로 귀결되지 않을까 생각이 듭니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation data load\n",
    "AUG_data = pd.read_csv('Augmented_data.csv')\n",
    "AUG_data = AUG_data.rename(columns={\"back_premise\": \"premise\", \"back_hypothesis\": \"hypothesis\"})\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_concated1['label_num'] = le.fit_transform(df_concated1['label'])\n",
    "df_concated2['label_num'] = le.transform(df_concated2['label'])\n",
    "AUG_data['label_num'] = le.transform(AUG_data['label'])\n",
    "\n",
    "# Dataframe convert to List\n",
    "KorNLU_list = [df_concated2.iloc[i] for i in range(len(df_concated2))] \n",
    "AUG_list = [AUG_data.iloc[i] for i in range(len(AUG_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [10:33<00:00,  1.44it/s]\n",
      "100%|██████████| 44/44 [00:18<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1/5    epoch : 1/100    time : 658/65122 \n",
      "TRAIN acc : 0.78067 \n",
      "VALID acc : 0.92430    best : 0.92430\n",
      "==> best model saved - 1 epoch / 0.92430    difference 0.42430 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [10:20<00:00,  1.47it/s]\n",
      "100%|██████████| 44/44 [00:18<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1/5    epoch : 2/100    time : 645/63230 \n",
      "TRAIN acc : 0.87331 \n",
      "VALID acc : 0.92637    best : 0.92637\n",
      "==> best model saved - 2 epoch / 0.92637    difference 0.00207 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [10:19<00:00,  1.47it/s]\n",
      "100%|██████████| 44/44 [00:18<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1/5    epoch : 3/100    time : 642/62250 \n",
      "TRAIN acc : 0.91737 \n",
      "VALID acc : 0.91797    best : 0.92637\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 466/914 [05:16<05:01,  1.49it/s]"
     ]
    }
   ],
   "source": [
    "# KFold\n",
    "fold=0\n",
    "folds = []\n",
    "k_acc_plot, k_val_acc_plot = [], []    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "kf = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "for train_idx, valid_idx in kf.split(df_concated1):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "\n",
    "    Train_set = [df_concated1.iloc[i] for i in train_idx]\n",
    "    Valid_set = [df_concated1.iloc[i] for i in valid_idx]\n",
    "    \n",
    "    # KLUE + KorNLU + Back Translation\n",
    "    Train_set = Train_set + AUG_list + KorNLU_list\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    train_set = NLIDataset(data=Train_set, \n",
    "                            max_seq_len=max_seq_len,    \n",
    "                            tokenizer=tokenizer)\n",
    "    Train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "    \n",
    "    # Validation \n",
    "    valid_set = NLIDataset(data=Valid_set, \n",
    "                            max_seq_len=max_seq_len, \n",
    "                            tokenizer=tokenizer)\n",
    "    Valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "        \n",
    "    model = Model_library.from_pretrained(model_name, num_labels=num_labels).to(device) \n",
    "    model = nn.DataParallel(model).to(device)\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = drop_out\n",
    "        \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "        'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n",
    "        'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    loss_fn = SmoothCrossEntropyLoss(smoothing=label_smoothing)\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    best=0.5\n",
    "    acc_plot, val_acc_plot = [], []\n",
    "    for e in range(epochs):\n",
    "        start=time.time()\n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for batch_id, batch in tqdm(enumerate(Train_loader), total=len(Train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            ids = torch.tensor(batch['input_ids'], dtype=torch.long, device=device)\n",
    "            segment_ids = torch.tensor(batch['token_type_ids'], dtype=torch.long, device=device)\n",
    "            atts = torch.tensor(batch['attention_mask'], dtype=torch.float, device=device)\n",
    "            labels = torch.tensor(batch['labels'], dtype=torch.long, device=device)\n",
    "            with torch.cuda.amp.autocast():    \n",
    "                pred = model(input_ids=ids, token_type_ids=segment_ids, attention_mask=atts)[0] \n",
    "                pred = pred.type(torch.FloatTensor).to(device)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_acc += calc_accuracy(pred, labels)\n",
    "        train_acc = train_acc/(batch_id+1)\n",
    "        acc_plot.append(train_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        for batch_id, batch in tqdm(enumerate(Valid_loader), total=len(Valid_loader)):\n",
    "            with torch.no_grad():\n",
    "                ids = torch.tensor(batch['input_ids'], dtype=torch.long, device=device)\n",
    "                segment_ids = torch.tensor(batch['token_type_ids'], dtype=torch.long, device=device)\n",
    "                atts = torch.tensor(batch['attention_mask'], dtype=torch.float, device=device)\n",
    "                labels = torch.tensor(batch['labels'], dtype=torch.long, device=device)\n",
    "                pred = model(input_ids=ids, token_type_ids=segment_ids, attention_mask=atts)[0]\n",
    "                pred = pred.type(torch.FloatTensor).to(device)\n",
    "            valid_acc += calc_accuracy(pred, labels)\n",
    "        valid_acc = valid_acc/(batch_id+1)\n",
    "        val_acc_plot.append(valid_acc)\n",
    "        \n",
    "        print_best = 0    \n",
    "        if valid_acc>=best:\n",
    "            difference = valid_acc - best\n",
    "            best = valid_acc \n",
    "            best_idx = e+1\n",
    "            model_state_dict = model.module.state_dict() if torch.cuda.device_count() > 1 else model.module.state_dict()\n",
    "            best_model_wts = copy.deepcopy(model_state_dict)\n",
    "            \n",
    "            # load and save best model weights\n",
    "            model.module.load_state_dict(best_model_wts)\n",
    "            torch.save(model_state_dict, model_save_folder+model_save+str(fold)+\".pt\")\n",
    "            print_best = '==> best model saved - %d epoch / %.5f    difference %.5f'%(best_idx, best, difference)\n",
    "            \n",
    "        TIME = time.time() - start\n",
    "        print(f'fold : {fold+1}/{n_fold}    epoch : {e+1}/{epochs}    time : {TIME:.0f}/{TIME*(epochs-e-1):.0f} ')\n",
    "        print(f'TRAIN acc : {train_acc:.5f} ')\n",
    "        print(f'VALID acc : {valid_acc:.5f}    best : {best:.5f}')\n",
    "        print('\\n') if type(print_best)==int else print(print_best,'\\n')\n",
    "        \n",
    "        if early_stopping.step(torch.tensor(valid_acc)):\n",
    "            break\n",
    "        \n",
    "    fold+=1    \n",
    "    k_acc_plot.append(max(acc_plot))\n",
    "    k_val_acc_plot.append(max(val_acc_plot))\n",
    "    \n",
    "print(\"Train Loss: \",np.mean(k_acc_plot),\", Valid Loss: \",np.mean(k_val_acc_plot))\n",
    "print(model_save + ' model is saved!')\n",
    "del train_set; del Train_loader; del valid_set; del Valid_loader; \n",
    "torch.cuda.empty_cache()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>1661</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1666 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index          label\n",
       "0         0  contradiction\n",
       "1         1        neutral\n",
       "2         2     entailment\n",
       "3         3  contradiction\n",
       "4         4  contradiction\n",
       "...     ...            ...\n",
       "1661   1661        neutral\n",
       "1662   1662        neutral\n",
       "1663   1663        neutral\n",
       "1664   1664        neutral\n",
       "1665   1665        neutral\n",
       "\n",
       "[1666 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test_data.csv\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "label_idx = dict(zip(list(le.classes_), le.transform(list(le.classes_))))\n",
    "idx_label = {value: key for key, value in label_idx.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# Test\n",
    "models = []\n",
    "for fold in range(n_fold):\n",
    "    model = Model_library.from_pretrained(model_name, num_labels=num_labels).to(device) \n",
    "    model = nn.DataParallel(model).to(device)\n",
    "    model_dict = torch.load(model_save_folder+model_save+str(fold)+\".pt\")\n",
    "    model.module.load_state_dict(model_dict) if torch.cuda.device_count() > 1 else model.load_state_dict(model_dict)\n",
    "    models.append(model)\n",
    "        \n",
    "# Test \n",
    "test_set = NLIDataset(data=test, \n",
    "                        max_seq_len=max_seq_len, \n",
    "                        tokenizer=tokenizer,\n",
    "                        mode='test')\n",
    "Test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "\n",
    "preds = predict(models,Test_loader,device)\n",
    "preds = np.array([idx_label[int(val)] for val in preds])\n",
    "\n",
    "sub[\"label\"] = preds\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_50000_5e-05_103_5_128_0.0001_0.1_5_fold is saved!\n"
     ]
    }
   ],
   "source": [
    "sub.to_csv(\"./RESULTS/{}.csv\".format(model_save), index=False)\n",
    "print(model_save + \" is saved!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ae36cb6ff022eff5efc58211fc54df341521a86255607fa35bbfdc97860af1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('COMPUTER_VISION_sy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
